# 4장 모델 훈련

## 4.1 선형 회귀

- 선형 회귀의 예측 모델
  $$
  \hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_k x_k
  $$
  

- 선형 회귀의 벡터 형태
  $$
  \hat{y} = \boldsymbol \theta^\intercal \cdot \bold x
  $$

- 회귀에서 가장 널리 사용되는 성능 측정 지표, 평균 제곱근 오차 (RMSE)

  RMSE를 최적화 하나 MSE 최적화 하나 결과는 같아서 덜 복잡한 MSE를 사용
  $$
  \text{MSE} = \frac{1}{N}\sum_{i=1}^{n}(\theta^\intercal \cdot \bold x_i - y_i)^2
  $$
  MSE를 최소화 하는 $\boldsymbol \theta​$ 를 찾는 것이 목표이다. 최소 자승 문제 (least square problem)라고 한다.
  $$
  \underset{\theta}{\arg\min} \sum_{i=1}^{n}(\theta^\intercal \cdot \bold x_i - y_i)^2
  $$

### 4.1.1 정규 방정식

$\theta​$ 를 찾기 위한 방식에는 두가지가 있다. 

- 해석적 방법: Normal Equation
- 수치 계산 방법: Gradient Descent

해석적 방식으로 푸는 방식에 대해 알아보자. 
$$
\begin{align}
X\theta & = y \\
X^\intercal X \theta & = X^\intercal y
\end{align}
$$
정방 행렬 $X^\intercal X​$ 의 역행렬 $(X^\intercal X)^{-1}​$이 존재 한다면 
$$
(X^\intercal X)^{-1}X^\intercal X \theta = (X^\intercal X)^{-1}X^\intercal y \\
\therefore \quad \theta = (X^\intercal X)^{-1}X^\intercal y
$$
 

### 4.1.2 계산 복잡도

- 정규 방정식은 $(n+1) \times (n+1)$ 크기의 $\boldsymbol X^\intercal \boldsymbol X$ 의 역행렬을 계산
- 일반적으로 $O(n^{2.4})$ 에서 $O(n^{3})$ 사이
- 훈련 세트의 샘플 수에는 선형적으로 증가

## 4.2 경사 하강법

특성이 매우 많고 훈련 샘플이 너무 많아 메모리에 모두 담을 수 없을 때 적합
$$
\bold x_{t+1} = \bold x_t -\alpha_t \nabla f(\bold x_i)
$$


### 4.2.1 배치 경사 하강법

매 경사 하강시 전체 훈련 세트에 대해서 계산.  이런 이유로 연산량이 많이 계산 속도가 느리다.

### 4.2.2 확률적 경사 하강법

하나의 샘플만 가져와 계산 하기 때문에 계산 속도는 빠르지만 최솟값에 찾아가는데 불안정하다. 부드럽게 감소 하지 않고 요동치면서 최적점을 향해 이동한다. 대신에 배치경사 하강법은 로컬 미니마에 빠질 수 있지만 이 불안정때문에 로컬 미니마에 걸려도 빠져나와서 전역 최솟값을 찾을 확률이 높다. 

### 4.2.3 미니배치 경사 하강법

임의의 작은 샘플 데이터셋에 대해 그래디언트를 계산하여 최솟값을 향해 하강하게 하는 방법이다. 배치 경가 하강법과 확률적 경가 하강법의 단점을 보완해준다.



- 세가지 방식의 이동 경로

![mlst_0410](/Users/kakao/Documents/notes/Study/Hands on ML/assets/mlst_0410-3517379.png)



- 학습률에 따른 경사 하강법

![0_QwE8M4MupSdqA3M4](/Users/kakao/Documents/notes/Study/Hands on ML/assets/0_QwE8M4MupSdqA3M4-3517721.png)

- 특성 스케일에 따른 경사 하강법

![normalized-vs-unnormalized](https://imaddabbura.github.io/img/gradient_descent_algorithms/normalized-vs-unnormalized.PNG)



- 전역 / 지역 최솟값

  ![mlst_0405](/Users/kakao/Documents/notes/Study/Hands on ML/assets/mlst_0405-3517384.png)

- 다양한 방식의 gradient descent 방식

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![opt1](/Users/kakao/Documents/notes/Study/Hands on ML/assets/opt1.gif) | ![opt2](/Users/kakao/Documents/notes/Study/Hands on ML/assets/opt2.gif) |



## 4.3 다항 회귀

각 특성들을 조합해 새로운 특성을 만들어 데이터셋에 포함하고 선형 모델로 훈련 시키는 방식. 기존 특성들을 활용하여 데이터셋을 잘 설명 할 수 있는 새로운 특성을 찾아 내는 것이 중요하다.

## 4.4 학습 곡선

특성이 너무 많아지면 학습 할 수 있는 변수들이 많아져 과적합이 발생한다. 얼마나 복잡한 모델을 사용 할 지 어떻게 정할 수 있을까? 어떻게 모델이 데이터에 과대적합 혹은 과소적합되었는지 판단 할 수 있을까?

- 교차 검증

- 학습 곡선

  모델마다 학습 곡선을 그려 원하는 퍼포먼스에 가깝고 검증데이터셋의 에러와 트레이닝 데이터셋의 에러의 오차가 작은 것이 큰 것보다 더 잘 학습 하였다고 볼 수 있다.

  ![994DFD4C5BAA0A4E04](/Users/kakao/Documents/notes/Study/Hands on ML/assets/994DFD4C5BAA0A4E04.png)

- Bias-Variance Tradeoff
  $$
  \text{E}\big[(y-\hat{f}(x))^2\big] = \text{E}\bigg[\hat{f}(x) - f(x)\bigg] + \text{E}\bigg[(\hat{f}(x) - \text{E}[\hat{f}(x)])^2\bigg] + \sigma^2
  $$
  

  에러는 Bias에러와 Variance에러로 decomposition 될 수 있다. 모델의 복잡도가 커지면 variance 에러가 커지고 bias 에러는 줄어든다. 하지만 모델의 복잡도가 줄어들면 bias가 커지고 variance 에러가 작아진다. 위 공식의 유도는 다음 링크를 참고 하자. https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff

  

## 4.5 규제(Regularization)가 있는 선형 모델

과대적합을 감소 시킬수 있는 한가지 방법은 모델이 학습 할수 있는 가중치에 제한을 두는 것입니다.

### 4.5.1 릿지 회귀 (Ridge Regression) 

가중치들의 제곱합을 최소화하는 제약 조건을 추가한 것이다. 하이퍼 모수 $\lambda$ 가 터지면 정규화이 정도가 커져 가중치 값들이 작아진다. $\lambda$ 가 0이 되면 일반 선형 회귀 모형이다.
$$
\hat{\boldsymbol \theta} = \underset{\boldsymbol \theta}{\arg\min} \bigg( \sum_{i=1}^{n}(\boldsymbol \theta^\intercal \cdot \bold x_i - y_i)^2  +\frac{1}{2} \alpha \sum_{j=1}^M \theta_j^2 \bigg )
$$

- 릿지회귀의 정규 방정식
  $$
  \hat{\boldsymbol \theta} = (\bold X^\intercal \cdot \bold X + \alpha\bold A)^{-1}\cdot \bold X^\intercal\cdot\bold y
  $$
  

### 4.5.2 라쏘 회귀 (Lasso Regression)

가중치의 절대값의 합을 최소화하는 것을 추가적인 제약 조건으로 준다. 영향력있는 특성에 가중치를 몰아주는 성질이 있어 차원축소에 사용 할 수 있다. 자동으로 특성을 선택하고 **sparse model**을 만든다.
$$
\hat{\boldsymbol \theta} = \underset{\boldsymbol \theta}{\arg\min} \bigg( \sum_{i=1}^{n}(\boldsymbol \theta^\intercal \cdot \bold x_i - y_i)^2  + \alpha \sum_{j=1}^M |\theta_j| \bigg )
$$


- 릿지와 라쏘의 차이

![main-qimg-2a88e2acc009fa4de3edeb51e683ca02](/Users/kakao/Documents/notes/Study/Hands on ML/assets/main-qimg-2a88e2acc009fa4de3edeb51e683ca02.png)

### 4.5.3 엘라스틱넷 (Elastic Nets)

가중치의 절대값의 합과 제곱합을 동시에 제약 조건으로 가지는 모형
$$
\hat{\boldsymbol \theta} = \underset{\boldsymbol \theta}{\arg\min} \bigg( \sum_{i=1}^{n}(\boldsymbol \theta^\intercal \cdot \bold x_i - y_i)^2  + \alpha_1 \frac{1}{2} \sum_{j=1}^M \theta_j^2 + \alpha_2 \sum_{j=1}^M |\theta_j| \bigg )
$$
두개의 하이퍼 모수 $\alpha_1$, $\alpha_2$ 를 가진다.



### 4.5.3 조기 종료 (Early Stopping)

반복적인 학습 알고리즘을 규제하는 또 다른 방법은 검증에러가 최솟값에 도달했다고 판단되면 훈련을 중지 시키는 것이다. 학습이 진행됨에 따라 훈련세트에 대한 에러와 검증세트에 대한 에러가 줄어든다. 하지만 어느 순간 부터 검증에러는 상승하기 시작한다. 모형이 훈련데이터에 과대적합되기 시작한 것을 의미한다. 미니배치나 확률적 경사하강법에서는 곡선이 매끄럽니 않아 최솟값에 도달 했는지 확인 하기 어려울때는 최근 일정 에포크 동안 최솟값보다 크다면 과대적합으로 판단하고 학습을 멈출 수 있다. 그리고 검증에러가 최소 일때의 모델 파라미터를 선택한다.



## 4.6 로지스틱 회귀

회귀 알고리즘은 분류에서도 사용할 수 있다. 로지스틱 회귀는 샘플이 특정 클래스에 속할 확률을 추정하는데 사용된다.  결과 값이 $[-\infty, +\infty]$ 인 선형회귀에 로짓함수을 적용하여 결과 값을 0과 N사이의 실수로 변환한다. 종속변수가 특정한 구간내의 값만을 가지는 특성을 가진 경우에 로지스틱 회귀분석 방법으로 사용 할 수 있다. 확률을 추정 할 때 많이 사용한다.

### 4.6.1 확률 추정


$$
\sigma(t = \theta^\intercal \cdot \bold x) = \frac{1}{1 + e^{-t}}
$$
![1_RqXFpiNGwdiKBWyLJc_E7g](/Users/kakao/Documents/notes/Study/Hands on ML/assets/1_RqXFpiNGwdiKBWyLJc_E7g.png)

### 4.6.2 훈련과 비용 함수

양성 샘플($y=1$)에 대해서는 높은 확률로 추정하고 음성샘픙($y=0$)에 대해서는 낮은 확률로 추정하는 모델의 파라미터 $\theta$를 찾는것이다.

- 훈련 샘플에 대한 비용 함수

  $y$가 1일때 $-\log(\hat{p})​$ 가 0에 가까우면 비용은 커지고 1에 가까우면 비용은 0에 가깝다.

  $y$가 0일때 $-\log(1- \hat{p})$ 가 0에 가까우면 비용은 작아지고 1에 가까우면 비용은 커진다.
  $$
  c(\theta) = 
  \begin{cases}
  -\log(\hat{p}) & \text{when} \quad y = 1 \\
  -\log(1-\hat{p}) & \text{when} \quad y = 0
  \end{cases}
  $$

  | $-\log(\hat{p})$                                             | $-\log(1-\hat{p})$                                           |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | ![스크린샷 2019-03-26 오전 2.00.30](/Users/kakao/Documents/notes/Study/Hands on ML/assets/스크린샷 2019-03-26 오전 2.00.30.png) | ![스크린샷 2019-03-26 오전 2.00.54](/Users/kakao/Documents/notes/Study/Hands on ML/assets/스크린샷 2019-03-26 오전 2.00.54.png) |

- 전체 트레이닝 데이터 세트에 대한 비용 함수
  $$
  J(\theta) = -\frac{1}{N}\sum_{i=1}^N\bigg[ y_i\log(\hat{p}_i) + 
  (1-y_i)\log(1-\hat{p}_i) \bigg]
  $$
  close-form으로 계산하는 방법이 알려져 있지 않지만 볼록함수 이기 때문에 경사하강법으로 충분히 전역 최솟값을 보장한다.

### 4.6.3 결정 경계

### 4.6.4 소프트맥스 회귀

소프트맥스(softmax) 함수는 입력과 출력이 다변수인 함수이다. 출력의 합이 1이 되도록 하기 때문에 출력에 확률론적 의미를 부여할 수 있다. 다중클래스 분류시 여러개의 이진 분류기를 훈련 시켜 조합하는 방식이 아닌 직접 다중클래스에 대한 확률을 출력 한다.

- 클랙스 $k$ 에 대한 소프트맥스 점수
  $$
  s_k(\bold x) = \theta_k^\intercal \cdot \bold x
  $$
  각 클래스는 자신만의 파라미터 벡터 $\theta_k$가 있고 이 벡터들은 **parameter matrix** $\Theta$ 에 저장된다.

  샘플 $\bold x_i $에 대해 각 클래스에 대한 점수가 계산되면 소프트맥스 함수를 통과 시켜 클래스 $k$ 에 속할 확률 $\hat{p}_k$ 를 추정 할 수 있다. 

- 소프트맥스 함수

  - $\bold s(\bold x) $: 샘플 $\bold x$에 대한 각 클래스의 점수를 담고 있는 벡터

  $$
  \hat{p}_k = \sigma(\bold s(\bold x))_k = \frac{\exp(s_k(\bold x))}{\sum_{j=1}^K \exp{s_j(\bold x)}}
  $$

- 소프트맥스 회귀 분류기의 예측
  $$
  \begin{align}
  \hat{y} 
  & = \underset{k}{\arg\max} \ \ \sigma(\bold s(\bold x))_k \\
  & = \underset{k}{\arg\max} \ \ s_k(\bold x) \\
  & =  \underset{k}{\arg\max} \ \ \big( \theta_k^\intercal \cdot \bold x \big)
  \end{align}
  $$
  

- 훈련 방법

  모델이 타깃 클래스에 대해서 높은 확를을 다른 클래스에 대햇는 낮은 확률을 추정 하도록 훈련 시켜야 한다.

  **크로스 엔트로피**  비용 함수를 최소화 하는 방식으로 트레이닝 할 수 있다. 불확실성이 높다면 엔트로피가 높다.
  $$
  J(\Theta) = -\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K y_k^{(i)}\log \bigg(\hat{p}_k^{(i)}\bigg)
  $$
  $i$ 번째 샘플에 대한 타깃 클래스가 $k$ 일 때 $y_k^{(i)}$ 는 1이고 그 외에는 0이다. $K=2​$ 이면 로지스틱 회귀의 비용 함수와 같은 것을 알 수 있다.





































