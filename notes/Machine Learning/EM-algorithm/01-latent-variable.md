## Latent Variable

잠재변수 (latent or hidden variable)는 데이터에서 보여지지 않는 변수이다. 잠재 변수의 개념이 왜 확률 기반 모델에 필요 한지 알아보자. 

다음 예제를 생각해보자. 

| 예비 면접 | 고등학교 성적 | 대학교 성정 | IQ   | 화상인터뷰 점수 | 최종 점수 |
| --------- | ------------- | ----------- | ---- | --------------- | --------- |
| John      | 4.0           | 4.0         | 120  | 3/4             | ?         |
| Helen     | 3.7           | 3.6         |      | 4/4             | ?         |
| Jack      | 3.2           |             | 112  | 2/4             | ?         |
| Emma      | 2.9           | 3.2         |      | 3/4             | ?         |

| 과거 면접 | 고등학교 성적 | 대학교 성정 | IQ   | 화상인터뷰 점수 | 최종 점수 |
| --------- | ------------- | ----------- | ---- | --------------- | --------- |
| Sophi     | 3.4           | 3.6         |      | 4/4             | 85/100    |
| ...       |               |             |      |                 |           |

어떤 회사가 온사이트 면접자들을 초대해야 하는데 모든 사람에게 기회를 줄수 없어서 몇명에게만 기회를 주려고 한다. 그래서 사용자의 스펙을 이용하여 Onsite Interview 점수를 예측하고 예측된 점수가 좋은 사람들만 초대를 하려고 한다. 스펙을 피쳐로 사용하여 Linear regression이나 Neural Network 모델을 통해 Onsite Interview 점수를 예측을 할수 있다. 하지만 두 가지 문제가 있다. 

1. 결측 데이터가 존재한다. 대학교를 나오지 않아 대학교 점수가 없다고 면접에서 제외할 수는 없다.
2. 예측한 값의 불확실성을 수치화 할 수 없다. 어떤 면접자의 예측 점수가 50점이고 불확실성이 작다면 초대하고 싶지 않고 불확실성이 크다면 해당 면접자에 대한 결론을 내리기 애매해서 초대해서 한번 더 면접을 봐볼 수 있다.

위 문제를 해결하기 위해 확률모형을 사용 할 수 있다. 우선 위 다섯가지의 변수들은 서로 모두에게 연관관계가 있다. 예를들어 고등학교 성적이 좋다면 IQ가 높고 대학 성적도 높을 거라고 생각 할 수 있기 때문이다. 서로 모두에게 연결되어 있기에 확률모형의 구조를 잘 파악 했다고 볼 수 없다. 유연하면서 최소한의 구조를 가진 확률 모형을 생각해보자. (구조라 함은 연관 관계?)

우선 한가지 방법은 아래와 모든 스펙의 조합을 구해서 해당 조합의 최종 점수가 주어진 수치일 확률을 부여하는 것이다.

| 고등학교 성적 | 대학교 성정 | IQ   | 화상인터뷰 점수 | 최종 점수 | 발생 확률 |
| ------------- | ----------- | ---- | --------------- | --------- | --------- |
| 1.0           | 1.0         | 1    | 0/4             | 1/100     | 0.001     |
| 1.0           | 1.0         | 1    | 0/4             | 2/100     | 0.0023    |
| ...           | ...         | ...  | ...             | ...       | ...       |
| 4.0           | 4.0         | 180  | 4/4             | 100       | 0.000001  |

굉장히 비효율적인 방법이다. 대신에 아래와 같이 다섯개의 모수들을 가진 모형을 가정하자.
$$
p(x_1, x_2, x_3, x_4, x_5) = \frac{\exp(-\bold w^\intercal \bold x)}{Z}
$$
학습할 5개의 변수들만 있도록 하여 모델 복잡도를 많이 낮추었다. 하지만 정규화 상수 $$Z$$ 가 문제이다. 

다음 예제를 통해 베이즈 정리에서 정규화 상수를 구하는 방법을 알아보자. 다음은 베이즈 정리이다.
$$
p(\theta \mid x) = \frac{p(x \mid \theta) p(\theta)}{p(x)} = \frac{p(x, \theta)}{p(x)}
$$

| 파라미터                               | Discrete                                                     | Continuous                                                   |
| -------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| $$\theta_1$$                           | $$ p(x) = \sum_{t_1}^{\theta_1}p(x, t_1) $$                  | $$ p(x) = \int_{-\infin}^{\infin}p(x, t_1) d\theta_1$$       |
| $$\theta_1, \theta_2$$                 | $$ p(x) = \sum_{t_1}^{\theta_1}\sum_{t_2}^{\theta_2}p(x, t_1, t_2) $$ | $$ p(x) = \int_{-\infin}^{\infin}\int_{-\infin}^{\infin}p(x, t_1) d\theta_1\theta_2 $$ |
| ...                                    | ...                                                          |                                                              |
| $$\theta_1, \theta_2,\dots, \theta_d$$ | $$ p(x) = \sum_{t_1}^{\theta_1}\sum_{t_2}^{\theta_2}\dots\sum_{t_d}^{\theta_d}p(x, t_1, t_2, \dots, t_d) $$ | $$ p(x) = \int_{-\infin}^{\infin}\int_{-\infin}^{\infin}\dots\int_{-\infin}^{\infin}p(x, t_1) d\theta_1\theta_2d\dots\theta_d $$ |

높은 차원의 모델(high-dimensional model) 일 수록 정규화 상수를 구하는 것은 어려워진다. 위 예제에서는 수억개인 모든 가능한 조합에 대해서 summation을 해야 하기 때문이다.

다른 방법은 "지성"이라는 0부터 100까지의 값을 갖는 새로운 잠재 변수를 하나 가정하는 것이다. 5가지 확률 변수들은 서로 연결되어 있지는 않고 지성은 각각의 확률 변수와 연결되어 있다. 연결은 non-deterministic하다.
$$
\begin{align}
p(x_1, x_2, x_3, x_4, x_5)
& = \sum_{I=1}^{100}p(x_1, x_2, x_3, x_4, x_5 \mid I)p(I) \\
& = \sum_{I=1}^{100}p(x_1 \mid I)p(x_2 \mid I)p(x_3 \mid I)p(x_4 \mid I)p(x_5 \mid I)P(I)
\end{align}
$$
5가지 피쳐끼리는 independent하기 때문에 두번째 식처럼 분해 할 수 있다.



**Pros**

- Simpler models (less edges)
- Fewer parameters
- Latend variables are sometimes meaningful

**Cons**

- Harder to work with



## Probabilistic Clustering

We better cluster data in a soft way because

- it allows to tune hyper parameters
  - 하드클러스트링은 트레이닝데이터와 밸리데이션 데이터에서 k가 클수록 log likelihood가 커진다. 하지만 소프트 클러스터링에서는 k가 커지는 어는 순간부터 log likelihood가 줄어 들기 시작한다.
  - TOOD: 위 사실은 추후에 왜 그런지 좀더 살펴 보자
- we can learn generative model of the data



## Gaussian Mixture Model











