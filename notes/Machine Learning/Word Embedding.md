# Table of Contents

1. 자연어와 단어의 분산 표현

   1. 자연어 처리란
      1. 단어의 의미
   2. 시소러스
      1. WordNet
      2. 시소러스의 문제점
   3. 통계 기반 기법
      1. 단어의 분산 표현
      2. 분포 가설
      3. 동시 발생 행렬
      4. 벡터 간 유사도
      5. 유사 단어의 랭킹 표시
   4. 통계 기반 기법 계산하기
      1. 상호 정보량
      2. 차원 감소
      3. SVD에 의한 차원 감소
      4. PTB 데이터 셋
      5. PTB 데이터 셋 평가

2. word2vec

   1. 추론 기반 기법과 신경망
      1. 통계 기반 기법의 문제점
      2. 추론 기반 기법의 개요
      3. 신경망에서의 단어 처리
   2. 단순한 word2vec
      1. CBOW 모델의 추론 처리
      2. CBOW 모델의 학습
      3. word2vec의 가중치와 분산 표현
   3. 학습 데이터 준비
      1. 맥락과 타깃
      2. 원핫 표현으로 변환
   4. CBOW 모델 구현
   5. word2vec 보충
      1. CBOW 모델과 확률
      2. skip-gram 모델
      3. 통계 기반 vs. 추론 기반
   6. 정리

3. word2vec 속도 개선

   1. word2vec 개선 1
      1. Embedding 계층
   2. word2vec 개선 2
      1. 은닉층 이후 계산의 문제점
      2. 다중 분류에서 이진 분류로
      3. 시그모이드 함수와 교차 엔트로피 오차
      4. 네거티브 샘플링
      5. 네거티브 샘플링의 샘플링 기법
   3. 개선판 word2vec 학습
      1. CBOW 모델 구현
      2. CBOW 모델 평가
   4. word2vec 남은 주제
      1. word2vec을 사용한 어플리케이션 예
      2. 단어 벡터 평가 방법
   5. 정리

   

   # Table of Contents

   1. 자연어와 단어의 분산 표현

      1. 자연어 처리란

         1. 단어의 의미

            단어는 의미의 최소 단위이다. 컴퓨터에게 단어의 의미를 이해 시켜야 한다. '단어의 의미'를 잘 파악하는 표현 방법에 관해 생각 해보자. 세가지 기법이 있다. **시소러스를 활용한 기법**, **통계 기반 기법**, **추론 기반 기법 (word2vec)** 이다.

      2. 시소러스

         시소러스란 기본적으로 유의어 사전을 말한다. 자연어 처리에 이용되는 시소러스에는 단어 사이의 **상위와 하위** 혹은 **전체와 부분** 등 더 세세한 관계 까지 그래프로 정의 해둘수 있다. 유의어 집합을 만든 다음, 단어들의 관계를 그래프로 표현하여 단어사이의 연결을 정의 할 수 있습니다. 

         1. WordNet

            가장 유명한 시소러스이다. 유의어를 얻거나 **단어 네트워크** 를 이용할 수 있다. 단어 사이의 유사도를 구할 수 있다.

         2. 시소러스의 문제점

            신조어에 따른 시대 변화에 대응하기 어렵다. 또한 시대에 따라 언어의 의미가 변하기도 한다. 사람을 쓰는 비용 또한 크다. 단어의 미묘한 차이를 표현할 수 없다. 예를 들어 빈티지와 레트로는 의미는 같지만 용법은 다르다. 이 문제들을 피하기 위해, **통계 기반 기법** 과 신경망을 사용한 **추론 기반 기법** 을 알아 본다. 단어의 의미를 자동으로 추출한다.

      3. 통계 기반 기법

         말뭉치 (corpus)를 사용한다. 말뭉치란 대량의 텍스트 데이터이다. 맹목적으로 수집된 데이터가 아니라 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 텍스트 데이터를 말한다. 말뭉치에는 자연어에 대한 사람이 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 지식이 포함되어 있다.

         말뭉치에는 텍스트 데이터의 단어 각각에 품사가 레이블링될 수도 있다. 이럴 경우 말뭉치는 트리로 가공되어 주어지는 것이 일반적이지만 해당 페이지에서는 단순한 텍스트 데이터로 주어졌다고 가정한다.

         1. 단어의 분산 표현

            색들에는 '코발트블루'나 '싱크레드' 같은 고유한 이름을 붙혀줄 수 있다. 반면, RGB라는 세가지 성분이 어떤 비율로 섞여 있느냐로 표현하는 방법도 있다. 전자는 색의 가짓수만큼의 이름을 부여하는 한편, 후자는 색을 3차원의 벡터로 표현 한 것이다. RGB같은 벡터 표현이 모든 색을 3개의 성분으로 간결하게 표현 할 수 있고 색을 더 정확하게 명시 할 수 있다. 색끼리의 관련성 (비슷한 색인지 여부 등)도 벡터 표현 쪽이 더 쉽게 판단 할 수 있다. 

            색을 벡터로 표현하듯 단어도 벡터로 표현 할 수 있을까? 우리가 원하는 것은 단어의 의미를 정확하게 파악 할 수 있는 벡터 표현이다. 이를 단어의 **분산 표현 (distributional representation)** 이라고 한다. **단어의 분산 표현을 어떻게 구축 할 것인가가 앞으로 살펴볼 중요한 주제이다.**

         2. 분포 가설

            단어 벡터 표현의 연구는 많이 이루어져 왔고 거의 모두가 하나의 간단한 아이디어에 뿌리를 두고 있다. **단어의 의미는 주변 단어에 의해 형성된 것** 이라는 것이다. 이를 **분포 가설 (distributional hypothesis)** 라고 한다. 즉, 단어 자체에는 의미가 없고 그 단어가 사용된 **맥락 (context)** 이 의미를 형성한다는 것이다. 맥락이란 특정 단어 중심에 둔 그 주변 단어들을 말한다. 윈도우 사이즈로 주변 단어의 넓이를 정의 할 수 있다. 

            > 상황에 따라 왼쪽 단어만 혹은 오른쪽 단어만을 사용할 수도 있고, 문장의 시작과 끝을 고려 할수도 있다.

         3. 동시 발생 행렬 (co-occurrence matrix)

            분포 가설에 기초해 단어를 벡터로 나타내어 보자. 어떤 단어에 주목했을 때, 주변에 어떤 단어가 몇번 등장하는지를 세어 집계하는 방법이다. 이를 **통계 기반 (statistical based)** 기법 이라고 한다. 

            예를 들어, "you say goodbye and i say hello." 라는 문장이 있다면 다음과 표현 될 수 있다. 아래는 모든 단어에 대해 동시 발생하는 단어를 펴에 정리 한 것이다. **동시발생 행렬 (co-occurrence matrix)** 라고 한다. 모든 단어의 각각의 맥랙에 해당하는 단어의 빈도를 세어 표로 정리 한 것이다.

            |             | you  | say  | goodbye | and  |  i   | hello |  .   |
            | :---------: | :--: | :--: | :-----: | :--: | :--: | :---: | :--: |
            |   **you**   |  0   |  1   |    0    |  0   |  0   |   0   |  0   |
            |   **say**   |  1   |  0   |    1    |  0   |  1   |   1   |  0   |
            | **goodbye** |  0   |  1   |    0    |  1   |  0   |   0   |  0   |
            |   **and**   |  0   |  0   |    1    |  0   |  1   |   0   |  0   |
            |    **i**    |  0   |  1   |    0    |  1   |  0   |   0   |  0   |
            |  **hello**  |  0   |  1   |    0    |  0   |  0   |   0   |  1   |
            |    **.**    |  0   |  0   |    0    |  0   |  0   |   1   |  0   |

         4. 벡터 간 유사도

            동시발생 행렬을 활용해 단어를 벡터로 표현하는 방법을 알았다. 벡터 사이의 유사도를 측정하여 단어간의 유사도를 계산 할 수 있다. 여기서는 **코사인 유사도 (cosine similarity)** 를 사용해보자. 다음과 같이 정의 된다.
            $$
            \text{similiarity}(\vec x, \vec y) = \frac{\vec x \cdot \vec y}{|| \vec x || \cdot ||\vec y||} = \frac{x_1y_1 + \cdots + x_ny_n}{\sqrt{x_1^2 + \cdots + x_n^2} \sqrt{y_1^2 + \cdots + y_n^2}}
            $$
            ​	벡터를 정규화 하고 내적한 것이다. 코사인 유사도를 직관적으로 풀어보면 **두 벡터가 가리키는 방향이 얼마나 비슷한가** 이다. 방향이 완전히 같다면 1 완전이 다르다면 0이다.

         5. 유사 단어의 랭킹 표시

            코사인 유사도를 통해 어떤 단어가 주어지면 그 단어와 비슷한 단어를 유사도 순으로 랭킹을 매길수 있다.

      4. 통계 기반 기법 계산하기

         1. 상호 정보량 PMI (Pointwise Mutual Information)

            동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타낸다. 하지만 이 **발생** 횟수라는 것은 좋은 특징이 아니다. 의미 없는 고빈도 단어들 때문이다. 예를 들어 car와 drive의 동시 발생 횟수보다 the 와 car의 동시 발생 횟수가 더 높아 관련성이 더 높다고 판단 될 것이다. 이 문제를 해결 하기 위해 **점별 상호정보량 (Pointwise Mutual Information)** 이라는 척도를 사용한다. 다음과 같이 정의된다. $$p(x)$$는 x가 일어날 확률, $$p(y)$$ 는 y가 일어날 확률, $$p(x, y)$$ 는 $$x$$ 와 $$y$$ 가 동시에 일어날 확률이다. PMI가 높으면 관련성이 높다는 의미이다.
            $$
            \text{PMI}(x, y)= \log_2\frac{p(x, y)}{p(x)p(y)}
            $$
            자연어에 적용하면 $$p(x)$$ 는 단어 $$x$$ 가 말뭉치에 등장할 확률을 말한다. 

            말뭉치의 단어 수 $$N$$ 을 10,000이라 하고 the와 car와 drive가 각각 1,000번, 20번, 10번 등장했다고 보고 the와 car의 동시 발생수는 10회, car와 drive의 동시 발생 수는 5회라고 가정해보자. PMI(the, car) 와 PMI(car, drive)를 계산 해보자.
            $$
            \begin{align}
            \text{PMI(the, car)} & = \log_2 \frac{10 \cdot 10000}{1000\cdot 20} \approx 2.3 \\ \\
            \text{PMI(car, drive)} & = \log_2 \frac{5 \cdot 10000}{20 \cdot 10} \approx 7.97
            \end{align}
            $$
            단어가 단독으로 출현한 것 만큼 점수를 낮춰줘서 이제 car와 drive 가 관련성이 더 높다고 나온다. 

            두단어의 동시발생 횟수가 0이면 $$\log_2 0 = -\infin$$ 이다. 이를 보완하기 위해 다음과 같이 **양의 상호정보량 PPI (Positive PMI)** 를 사용 할수 있다.
            $$
            \text{PPMI}(x, y) = \max(0, \text{PMI}(x, y))
            $$
            PPMI 행렬에도 여전히 큰 문제가 있다. 말뭉치의 어휘수가 증가함에 따라 각 단어 벡터의 차원 수도 증가하는 것이다. 또한, PPMI 행렬을 희소행렬이다. 각 원소의 중요도가 낮다는 것이다. 이런 벡터는 노이즈에 약하고 견고하지 못하다. 이 문제를 해결하고자 차원 축소를 알아보자.

         2. 차원 감소 Dimentionality Reduction (TODO: 보충)

            벡터의 차원을 줄이는 방법이다. 단순히 줄이는 것이 아닌 중요한 정보는 최대한 유지하고 줄이는 것이 핵심이다. 데이터의 분포를 고려해 중요한 축을 찾는 것이다. 차원 감소의 핵심은 희소벡터에서 중요한 축을 찾아내어 더 적은 밀집 벡터로 다시 표현 하는 것이다. 밀집 벡터야 말로 우리가 찾는 단어의 분산 표현이다. 

            차원을 감소시키는 방법에는 여러가지가 있지만 여기서는 **특이값 분해 Singular Value Decomposition** 을 사용한다. SVD는 임의의 행렬을 세 행렬의 곱으로 분해하며 다음과 같다.
            $$
            X = USV^\text T
            $$
            $$U$$ 와 $$V$$ 는 직교 행렬 (orthogonal matrix) 이다. 즉, 열벡터는 서로 직교한다. $$S$$ 는 대각 행렬 (diagonal matrix) 이다. 

         3. SVD에 의한 차원 감소

         4. PTB 데이터 셋

         5. PTB 데이터 셋 평가

         6. 정리

            말뭉치를 이용해 맥락에 속한 단어의 등장 횟수를 센 후 PPMI 행렬로 변환하고 , 다시 SVD를 이용해 차원을 감소시킴으로 더 좋은 밀집 단어 벡터를 얻어 냈다. 이것이 단어의 분산 표현이고, 각 단어는 고정 길이의 밀집 벡터로 표현되었다. 분산 표현에 따르면 의미가 비슷한 단어들이 벡터 공간에서도 서로 가까이 모여있음을 코사인 유사도를 통해 알 수 있다. 

   2. word2vec

      이전 장에서는 통계 기반 기법으로 단어의 분산 표현을 얻었다면 이번 장에서는 더 강력한 추론 기반 기법으로 분산 표현을 얻어보자. 추론 과정에서 신경망을 이용한다. 

      1. 추론 기반 기법과 신경망

         통계 기반 기법과 추론 기반 기법에서 단어의 의미를 얻는 방식은 다르지만, 배경에는 모두 분포 가설이 있다.

         1. 통계 기반 기법의 문제점

            통계 기반 기법에서는 주변 단어의 빈도를 기초로 단어를 표현했다. 구체적으로는 말뭉치의 동시발생 행렬을 만들고 SVD를 적용하여 밀집 벡터 (단어의 분산 표현)을 얻었다. 하지만 해당 방식은 대규모 말뭉치를 다룰 때 문제가 발생한다.

            어휘가 100만개라면 통계 기반 기법에서는 100만 X 100만 행렬을 만든다. 이런 거래 행렬에 SVD를 적용하는 건 쉽지 않다.

            > SVD를 $$n\times x$$ 행렬에 적용하는 시간 복잡도는 $$O(n^3)$$ 이다. 실제로는 근사적인 기법과 희소행렬의 성질을 이용해서 속도를 개선 할 수는 있지만 여전히 상당한 컴퓨팅 자원을 잡아먹는다. 

            통계 기반 기법은 말뭉치 전체의 동시 발생 행렬과 PPMI등을 한번의 처리(배치 학습)만으로 단어의 분산 표현을 얻는다. 추론 기반 기법에서는 미니배치 방식으로 소량의 학습 샘플을 반복해서 학습하면 가중치를 갱신해간다. 두 기법은 큰 차이를 보인다. 병렬 계산도 가능해서 학습 속도를 높일수도 있다.

         2. 추론 기반 기법의 개요

            추론 기반 기법에서는 추론이 주된 작업니다. 주변단어(맥락)가 주어졌을 때 그 사이에 무슨 단어가 들어가는지 추측하는 작업이다. 예를 들어 "you ? goobye and I say hello." 에서 ?에 무슨 단어가 들어가는지 추론 하는 것이다. 이런 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습하는 것이다. 즉, 추론 기반 기법에서는 맥락을 입력하면 학습된 모델은 각 단어의 출현 확률을 출력한다. 우리는 여기서 모델로 신경망을 사용하는 것이다.

         3. 신경망에서의 단어 처리

            신경망은 단어 자체로 처리 할 수 없으므로 단어를 고정길이의 벡터로 변환해야 한다. 대표적인 방법이 one-hot vector로 사용 하는 것이다. 먼저 총 어휘 수만큼의 원소를 갖는 벡터를 만들고 인덱스가 단어 ID와 같은 원소를 1로 나머지는 0으로 하는 것이다.       

      2. 단순한 word2vec

         이제 맥락을 입력하면 각 단어의 출현 확률을 출력하는 모델을 구현 할 차례이다. 여기서 사용할 신경망은 **CBOW (continuous bag-of-words)** 이다. 

         1. CBOW 모델의 추론 처리

            CBOW 모델은 맥락으로 부터 타깃 단어를 추측하는 신경망이다 (타깃은 중앙단어 이고 그 주변 단어들이 맥락). CBOW을 가능한 한 정확하게 추론하도록 훈련 시켜서 단어의 분산 표현을 얻는 것이다. CBOW의 입력은 맥락이다.먼저 맥락을 원핫 벡터로 변환하여 CBOW 모델이 입력으로 받을수 있도록 한다.  

            ![cbow1](/Users/kakao/Documents/notes/Machine Learning/assets/cbow1.png)

            입력층이 2개가 있고 은닉층은 완전연결계층에 의해 변환된 값이 되는데 입력층이 여러개 이면 전체를 평균하면 된다. 출력층의 뉴런 하나하나가 각각 단어의 점수를 뜻한다. 값이 높을수록 대응 단어의 출현 확률도 높아진다. 점수란 소프트맥스를 통해서 확률로 해석되기 전의 값이다. 엄밀히 말하면 소프트맥스 계층을 통과한 뉴런을 출력층이라고 한다. $$W_{V\times N}$$의 각 행이 해당 단어의 분산표현이 담겨 있다고 볼수 있다. 학습을 진행하면서 맥락에서 출현하는 단어를 잘 추측하는 방향으로 이 분산 표현들이 갱신된다. 이것이 word2vec의 전체 그림이다.

            은닉층의 뉴런수를 입력층의 뉴런 수보다 적게 하는 것이 핵심이다. 은익층에는 단어 예측에 필요한 정보를 간결하게 밀집벡터로 응축 시킬수 있다. 은익층의 정보는 인간이 이해 할 수 없는 코드로 되어져 있다. 이는 인코딩에 해당하고 은닉층으로부터 원하는 결과를 얻는 작업은 디코딩이다. 디코딩이란 인코딩된 정보를 인간이 이해 할 수 있는 표현으로 복원하는 것이다.

            CBOW 모델은 활성화 함수를 사용하지 않는 간단한 구성의 신경망이다. 뒤에서 나오는 소프트맥스 부분만이 비선형 변환이다. (TODO: 왜 히든 레이어는 선형 활성화 함수를 하는지?? 단순히 연산량을 줄이기 위해서??)

         2. CBOW 모델의 학습

            지금까지의 CBOW 모델은 출력층에서 각 단어의 점수를 출력했다. 이제 소프트맥스 함수를 적용하여 확률로 변환하자. 이 확률은 맥락이 주어졌을 때 그 중앙에 각각의 단어가 출현할 확률을 나타낸다. CBOW 모델은 학습에 사용되는 말뭉치가 다르면 학습 후 얻게 되는 단어의 분산 표현도 달라진다. 

            ![cbow2](/Users/kakao/Documents/notes/Machine Learning/assets/cbow2.png)

            현재 다루고 있는 CBOW 모델은 다중 클래스를 분류하는 신경망이다. 신경망을 학습하려면 소트프맥스와 교차 엔트로피 오차만 이용하면 된다. 소프트맥스를 적용하여 점수를 확률로 변환하고 확률과 정답 레이블로부터 교차 엔트로피를 구한 후 해당 값을 손실로 사용해서 학습을 진행한다. 

         3. word2vec의 가중치와 분산 표현

            CBOW 모델에는 두가지 가중치가 있다. 입력 측 가중치 $$W_{\text {in}}$$ 의 각 행이 각 단어의 분산 표현이다. 출력 측 가중치 $$W_{\text{out}}$$  에도 단어의 의미가 인코딩된 벡터가 저장되고 있다고 생각 할 수 있다. 출력 측 가중치는 각 단어의 분산 표현이 열 방향으로 저장된다. 그럼 최종적으로 이용하는 단어의 분산 표현으로는 어느 쪽 가중치를 선택해야 할까? 두개를 단순히 더하든 어떻게든 조합해서 사용해야 할까? word2vec에서는 대중적으로 입력 측 가중치를 선택한다. (TODO: 이유는??) 여기서도 $$W_{\text{in}}$$ 을 단어의 분산 표현으로 사용한다.

            > word2vec의 skip-gram에서 $$W_{\text{in}}$$ 의 효과를 실험을 통해 보여준다. word2vec과 비슷한 GloVe 에서는 두 가중치를 더했을 때 좋은 결과를 얻는 것을 보여준다. 

      3. 학습 데이터 준비

         1. 맥락과 타깃

            우리가 해야 할 일은 맥락을 입력 했을 때 타깃 (중앙 단어)이 출현한 확률을 높이는 것이다.

         2. 원핫 표현으로 변환

      4. CBOW 모델 구현

      5. word2vec 보충

         CBOW 모델을 확률 관점에서 다시 살펴보자.

         1. CBOW 모델과 확률

            CBOW 모델을 확률 표기법으로 표현해보자. CBOW 모델이 하는 일은 맥락을 주변 타깃 단어가 출현할 확률을 출력하는 것이다. 말뭉치를 다음과 같은 시퀀스로 표현하고 $$t$$ 번째 단어에 대해 윈도우 크기가 1인 맥락을 고려 하겠다.
            $$
            w_1\space w_2\space w_3 \space \cdots \space w_{\scriptsize{\text t}-1} \space w_t \space w_{\scriptsize{\text t}+1} \space \cdots \space w_{\scriptsize{\text{T}-1}} \space w_{\scriptsize{\text{T}}}
            $$
            맥락으로 $$w_{\scriptsize{\text t}-1}$$ 와 $$w_{\scriptsize{\text t}+1}$$ 가 주어지면 $$w_{\scriptsize \text t}$$ 가 될 확률은 수식으로 다음과 같이 사후확률을 사용해 쓸수 있다.
            $$
            p(w_{\scriptsize \text t} \mid w_{\scriptsize\text t-1}, w_{\scriptsize\text t + 1})
            $$
            해당 식을 사용하여 손실 함수도 표현 할 수 있다. 교차 엔트로피 오차를 적용한다. 교차 엔트로피는 다음과 같다.
            $$
            L = - \sum_k t_k \log y_k
            $$
            $$y_k$$는 $$k$$ 번째의에 해당하는 사건이 일어날 확률이고  $$t_k$$ 는 정답 레이블이며 원핫 벡터로 표현된다. 여기서 $$w_t$$ 만 발생이므로 $$w_t$$에 해당하는 원소만 1이고 나머지는 0이다. 이 점을 감안하면 다음과 같이 식을 유도 할 수 있다.
            $$
            L = -\log P(w_t \mid w_{t-1}, \space w_{t+1})
            $$
            이를 **negative log likelihood** 라고 하고 샘플 데이터 하나에 대한 손실 함수이다. 말뭉치 전체로 확장하면 다음과 같다. 
            $$
            L = -\frac{1}{T} \sum_{t=1}^T \log P(w_t \mid w_{t-1},\space w_{t+1})
            $$
            CBOW 모델의 학습이 수행하는 일은 위의 손실함수 값을 가능한 작게 만드는 것이다. (마이너스를 제거하고 $$\arg\max$$ 로 풀수도 있다.)
            $$
            \arg\min_W -\frac{1}{T} \sum_{t=1}^T \log P(w_t \mid w_{t-1},\space w_{t+1}; W)
            $$
            여서 윈도우 크기를 1로 사용 하였지만 다른 크기로 사용해서 수식으로 표현 할 수 있다.

         2. skip-gram 모델

            skip-gram 모델은 CBOW에서 다루는 맥락과 타깃을 역전 시킨 모델이다. "? say ? and I say hello." 에거 타깃 단어를 이용해 주변 단어 (맥락)들을 추측하는 모델이다. 

            ![skipgram](/Users/kakao/Documents/notes/Machine Learning/assets/skipgram.png)

            입력층은 하나이고 출력층은 맥락의 수만큼 존재한다. 출력층에서는 개별적으로 손실을 구하고, 이 개별 손실들을 모두 더한 값을 최종 손실로 한다. 

            skip-gram 모델의 확률 표기는 다음과 같다.
            $$
            P(w_{t-1}, \space w_{t+1} \mid w_t)
            $$
            skip-gram 모델에서는 맥락의 단어들 사이에 연관성이 없다고(정확히는 조건부 독립) 가정을 하기 때문에 다음과 같이 분해 할 수 있다.
            $$
            P(w_{t-1}, \space w_{t+1} \mid w_t) = P(w_{t-1} \mid w_t) P(w_{t+1} \mid w_t)
            $$
            교차 엔트로피를 적용하여 skip-gram 모델의 손실함수를 유도 할 수 있다.
            $$
            \begin{align}
            L \quad & = \quad -\log P(w_{t-1}, \space w_{t+1} \mid w_t) \\
            \quad & = \quad  - \log P(w_{t-1} \mid w_t) P(w_{t+1} \mid w_t) \\
            \quad & = \quad - \big(\log P(w_{t-1} \mid w_t)  +  \log P(w_{t+1} \mid w_t) \big)
            \end{align}
            $$
            말뭉치 전체로 확장하면,
            $$
            L = -\frac{1}{T}\sum_{t=1}^T \big(\log P(w_{t-1} \mid w_t)  +  \log P(w_{t+1} \mid w_t) \big)
            $$
            skip-gram 모델의 학습이 수행하는 일은 위의 손실함수 값을 가능한 작게 만드는 것이다. (마이너스를 제거하고 $$\arg\max$$ 로 풀수도 있다.)
            $$
            \arg\min_W -\frac{1}{T}\sum_{t=1}^T \big(\log P(w_{t-1} \mid w_t)  +  \log P(w_{t+1} \mid w_t) \big)
            $$

            >CBOW 모델과 skip-gram 모델 중 어느 것을 사용 해야 할까? skip-gram 모델이 분산 표현의 정밀도 면에서 더 좋은 경우가 많다. 특히 말뭉치가 커질수록 저빈도 단어나 유추문제의 성능면에서 더 띄어났다. 반면, 학슥 속도 면에서는 CBOW 모델이 빠르다. skip-gram 모델은 손실을 맥락수 만큼 구해야 해서 계산 비용이 그만큼 커진다. skip-gram 모델이 더 어려운 문제를 푸는 것이라서 분산 표현이 더 뛰어날 가능성이 커진다. 예를 들어 CBOW에서는 우리는  say라고 쉽게 유추 가능 하지만 skip-gram 문제는 여러가지 후보가 떠오르기 때문이다.

         3. 통계 기반 vs. 추론 기반

            통계 기반 기법 LSA (latent semantic analysis)은 말뭉치의 전체 통계로부터 1회 학습하여 단어의 분산 표현을 얻었다. 반면 추론 기반 기법에서는 말뭉치를 일부분씩 여러번 보면서 미니배치방식으로 학습하였다.

            어휘에 추가할 새로운 단어가 생겨서 단어의 분산 표현을 갱신해야 하는 상황을 생각해보자. 통계 기반 기법에서는 단어-문맥(동시발생) 행렬을 다시 만들고 SVD통한 차원축소를 다시 해야 합니다. 하지만 추론 기반 기법 (word2vec)은 기존에 학습한 경험을 초기화 하지 않고 기존에 학습한 가중치를 초깃값으로 사용해 다시 학습 할 수 있다. 

            통계 기반 기법에서는 주로 단어의 유사성이 인코딩이되지만 word2vec (특히, skip-gram) 에서는 단어의 유사성은 물론 단어 사이의 패턴까지도 파악되어 인코딩 된다. word2vec은 king-man+woman=queen 같은 유추 문제를 풀수 있다. 하지만  의외로 추론 기반과 통계 기반 기법의 우열을 가릴수 없다고 한다.(TODO:  좀더 자세히 알아보자.)

            **추론 기반 기법과 통계 기반 기법은 서로 관련되어 있다.** skip-gram과 네거티브 샘플링을 이용한 모델은 단어-문맥 행렬(실제로는 살짝 다르게 수정)에 행렬분해를 적용한 것과 같다. 두세계는 특정 조건하에 서로 연결되어 있다. GloVe 이 추론기반과 통계 기반을 융합한 기법이다. 말뭉치 전체의 통계 정보를 손실 함수에 도입했다. 

      6. 정리

         - 추론 기반 기법은 추측하는 것이 목적이며, 부산물로 단어의 분산 표현을 얻을 수 있다.
      - word2vec은 추론 기반 기법이며, 단순한 2등 신경망이다.
         - word2vec은 skip-gram과 CBOW 모델을 제공한다.
         - CBOW 모델은 여러 단어(맥락)로부터 하나의 단어(타깃)를 추측한다.
         - 반대로 skip-gram 모델은 하나의 단어(타깃)로부터 다수의 단어(맥락)을 추축한다.
         - word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산표현 갱신이나 새로운 단어 추가를 효율적으로 수행 할 수 있다.
   
   3. word2vec 속도 개선
   
      CBOW 모델은 단순한 2층 신경망이라서 간단하게 구현 할 수 있었다. 하지만 말뭉치에 포함되 어휘수가 많아지면 계산량도 커진다는 것이다. 
      
      단순한 word2vec에 두가지 개선을 추가 한다. Embedding이라는 새로운 계층을 도입한다. 두번째로 네거티브 샘플링이라는 새로운 손실함수를 도입한다. 
      
      어휘수가 많으면 다음의 두계산이 병목이 된다. 
      
      > 1. 입력층의 원핫벡터와 가중치 행렬 $$W_{\text{in}}$$ 의 곱 계산
      > 2. 은닉층과 가중치 행렬 $$W_{\text{out}}$$ 의 곱 및 소프트맥스 계층의 계산
   
      1. word2vec 개선 1
        
         1. Embedding 계층
         
            원핫벡터와 입력 가중치 $$W_{\text{in}}$$ 의 곱은 결과적으로 수행하는 일은 단지 행렬의 특정 행을 추출하는 것뿐이다. 사실상 원핫 벡터로의 변환과 입력 가중치의 행렬곱 계산은 사실 필요가 없는 것이다. 해당 부분을 우리는 단순히 단어 ID로 행(벡터)를 추출하는 계층으로 대신 할 수 있다. 해당 계층을 embedding 계층으로 부르자.
         
            > 자연어처리 분야에서 단어의 밀집벡터를 단어 임베딩 혹은 분산표현 (distributed representation) 이라 한다. 통계 기반으로 얻은 단어의 벡터는 distributional representation이라고 한다.
         
      2. word2vec 개선 2
        
         1. 은닉층 이후 계산의 문제점
         
            남은 병목은 은닉층 이후의 처리 (행렬 곱과 소프트맥스 계층)이다. 네거티브 샘플링을 통해 해당 병목을 해소해보자. 소프트맥스 대신 네거티브샘플링을 이용하면 어휘가 아무리 많아져도 계산량을 낮은 수준에서 일정하게 억제 할 수 있다.
         
         2. 다중 분류에서 이진 분류로
         
            다중 분류를 이진 분류로 근사하는 것이 네거티브 샘플링을 이해하는데 중요한 포인트이다. 지금까지는 맥락이 주어지면 정답이 되는 단어를 유추 하도록 하였습니다. 이제부터 맥락이 주어졌을 때 타깃단어는 모모 이다라고 대답하는 신경망을 생각해볼수 있다. 
         
         3. 시그모이드 함수와 교차 엔트로피 오차
         
            이진 분류 문제를 신경망으로 풀려면 점수에 시그모이드 함수를 적용해 확률로 변환하고 손실함수로는 교차 엔트로피 오차를 사용한다. 
         
         4. 네거티브 샘플링
         
            지금까지는 정답에 대해서만 학습을 했다. 가중치가 잘 학습이 되었다면 you와 goodbye가 문맥이고 say가 정답인 경우에는 1에 가까운 높은 확률을 출력할 것이다. 하지만 hello를 입력하면 어떤 확률이 나올까? 부정적인 예에 대해서는 어떠한 지식도 얻지 못했다. 우리가 학습하고자 하는 방향은 정답에 높은 확률을 출력하고 오답에는 0에 가까운 확률을 출력하는 것이다.
         
            모든 부정적인 예에 대해 학습을 시키기에는 어휘수가 크면 감당 하기 힘들다. 근사적인 해법으로 부정적인 예를 5개 혹은 10개를 샘플링하여 정답과 같이 손실을 구한다. 손실을 더한 값이 최종 손실이 되는 것이다.
         
         5. 네거티브 샘플링의 샘플링 기법
         
            부정적인 예를 어떻게 샘플링하느냐 인데 무작위로 샘플링 하는 것보다는 말뭉치에서 자주 등장하는 단어를 많이 샘플링하고 드물게 등장하는 단어를 적게 샘플링 하는 것이다. 말뭉치에서 단어의 출현 횟수로 확률 분포를 구하고 그 확률 분포대로 단어를 샘플링하면 된다.
         
      3. 개선판 word2vec 학습
         1. CBOW 모델 구현
         2. CBOW 모델 평가
      
      4. word2vec 남은 주제
         1. word2vec을 사용한 어플리케이션 예
      
            단어의 분산표현이 중요한 이유는 트랜스퍼 러닝에 있다. 텍스트 분류, 문서 클러스터링, 감정 분석 등 자연어 처리 작업이라면 가장 먼저 단어를 벡터로 변환하는 작업을 해야 하는데, 이때 미리 학습된 단어의 분산 표현을 이용 할 수 있다.
      
         2. 단어 벡터 평가 방법
      
            **학습된 단어의 분산 표현이 좋은지 어떻게 평가 할 수 있을까?** 사람이 만들어 놓은 유사성 점수와 비교를 하거나 사람이 풀은 유추 문제와 비교 할 수 있다.
      
      5. 정리
   
